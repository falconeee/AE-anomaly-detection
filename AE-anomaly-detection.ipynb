{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bb3d68",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "\n",
    "# Silenciar logs menos importantes do TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3d636",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GERAÇÃO DE DADOS SINTÉTICOS\n",
    "# -----------------------------------------------------------------------------\n",
    "def generate_data():\n",
    "    \"\"\"Gera um DataFrame com séries temporais multivariadas e anomalias injetadas.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_timesteps = 1000\n",
    "    timestamps = pd.to_datetime(pd.date_range('2023-01-01', periods=n_timesteps, freq='H'))\n",
    "\n",
    "    # Comportamento normal (ondas senoidais com ruído)\n",
    "    time = np.arange(n_timesteps)\n",
    "    v1 = np.sin(time / 20) + np.random.normal(0, 0.1, n_timesteps)\n",
    "    v2 = np.cos(time / 30) + np.random.normal(0, 0.1, n_timesteps)\n",
    "    v3 = np.sin(time / 15) * np.cos(time / 25) + np.random.normal(0, 0.1, n_timesteps)\n",
    "\n",
    "    df = pd.DataFrame({'V1': v1, 'V2': v2, 'V3': v3}, index=timestamps)\n",
    "    \n",
    "    # Injetar anomalias no período de teste\n",
    "    is_anomaly = pd.Series(np.zeros(n_timesteps, dtype=int), index=timestamps)\n",
    "    \n",
    "    # Anomalia 1: Pico súbito\n",
    "    df.loc['2023-02-10 00:00:00':'2023-02-10 02:00:00', 'V1'] += 3.5\n",
    "    is_anomaly.loc['2023-02-10 00:00:00':'2023-02-10 02:00:00'] = 1\n",
    "\n",
    "    # Anomalia 2: Mudança de nível\n",
    "    df.loc['2023-02-20 12:00:00':'2023-02-20 18:00:00', 'V2'] -= 2.0\n",
    "    is_anomaly.loc['2023-02-20 12:00:00':'2023-02-20 18:00:00'] = 1\n",
    "    \n",
    "    # Anomalia 3: Ruído excessivo\n",
    "    noise_period = df.loc['2023-02-01 06:00:00':'2023-02-01 10:00:00']\n",
    "    df.loc[noise_period.index, 'V3'] += np.random.normal(0, 1.0, len(noise_period))\n",
    "    is_anomaly.loc[noise_period.index] = 1\n",
    "\n",
    "    return df, is_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ad8cb",
   "metadata": {},
   "source": [
    "# Pre-process and create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b38c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PRÉ-PROCESSAMENTO E CRIAÇÃO DE JANELAS\n",
    "# -----------------------------------------------------------------------------\n",
    "def preprocess_and_window(data, window_size):\n",
    "    \"\"\"Normaliza e cria janelas deslizantes (sequências) a partir dos dados.\"\"\"\n",
    "    # Normalização\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # Criação das janelas\n",
    "    X = []\n",
    "    for i in range(len(data_scaled) - window_size + 1):\n",
    "        X.append(data_scaled[i:i + window_size])\n",
    "    \n",
    "    return np.array(X), scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4f0e9",
   "metadata": {},
   "source": [
    "# Build autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CONSTRUÇÃO DO MODELO AUTOENCODER LSTM\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_lstm_autoencoder(input_shape):\n",
    "    \"\"\"Cria e compila um modelo de Autoencoder LSTM.\"\"\"\n",
    "    # Encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # A dimensionalidade latente é 32\n",
    "    encoded = LSTM(128, activation='relu', return_sequences=False)(inputs)\n",
    "    encoded = RepeatVector(input_shape[0])(encoded) # Repete o vetor latente para a entrada do decoder\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = LSTM(128, activation='relu', return_sequences=True)(encoded)\n",
    "    decoded = TimeDistributed(Dense(input_shape[1]))(decoded)\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mae') # Mean Absolute Error é robusto a outliers\n",
    "    \n",
    "    autoencoder.summary()\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3775388",
   "metadata": {},
   "source": [
    "# Plot and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FUNÇÕES DE PLOTAGEM E AVALIAÇÃO\n",
    "# -----------------------------------------------------------------------------\n",
    "def plot_reconstruction_error(error_df, threshold):\n",
    "    \"\"\"Plota o erro de reconstrução e o limiar de anomalia.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(error_df.index, error_df['error'], label='Erro de Reconstrução')\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label='Limiar de Anomalia')\n",
    "    plt.title('Erro de Reconstrução ao Longo do Tempo')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Erro (MAE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_anomalies(data, anomaly_indices):\n",
    "    \"\"\"Plota a série temporal original destacando as anomalias detectadas.\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for col in data.columns:\n",
    "        plt.plot(data.index, data[col], label=col, alpha=0.7)\n",
    "    \n",
    "    # Destaca as anomalias detectadas\n",
    "    anomalies = data.iloc[anomaly_indices]\n",
    "    for col in anomalies.columns:\n",
    "        plt.scatter(anomalies.index, anomalies[col], color='red', marker='x', s=50, label=f'Anomalia em {col}')\n",
    "        \n",
    "    plt.title('Detecção de Anomalias na Série Temporal Multivariada')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Valor Normalizado')\n",
    "    \n",
    "    # Lidar com legendas duplicadas\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edace870",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000310b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARÂMETROS\n",
    "WINDOW_SIZE = 24 # Usar 24 horas de dados para prever o próximo passo\n",
    "TRAIN_RATIO = 0.7 # 70% dos dados para treino (período normal)\n",
    "\n",
    "# 1. Carregar e dividir os dados\n",
    "df, is_anomaly = generate_data()\n",
    "n_train = int(len(df) * TRAIN_RATIO)\n",
    "\n",
    "train_df = df.iloc[:n_train]\n",
    "test_df = df.iloc[n_train:]\n",
    "test_labels = is_anomaly.iloc[n_train:]\n",
    "\n",
    "print(f\"Tamanho do dataset de treino: {len(train_df)}\")\n",
    "print(f\"Tamanho do dataset de teste: {len(test_df)}\")\n",
    "\n",
    "# 2. Pré-processar e criar janelas\n",
    "# Treinamos o scaler APENAS com dados de treino para evitar data leakage\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df)\n",
    "test_scaled = scaler.transform(test_df)\n",
    "\n",
    "# Janelas de treino (somente dados normais)\n",
    "X_train, _ = preprocess_and_window(train_df, WINDOW_SIZE)\n",
    "\n",
    "# Janelas de teste (inclui anomalias)\n",
    "X_test, _ = preprocess_and_window(test_df, WINDOW_SIZE)\n",
    "\n",
    "print(f\"Shape das janelas de treino: {X_train.shape}\")\n",
    "print(f\"Shape das janelas de teste: {X_test.shape}\")\n",
    "\n",
    "# 3. Criar e treinar o modelo\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "autoencoder = create_lstm_autoencoder(input_shape)\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')]\n",
    ")\n",
    "\n",
    "# 4. Calcular erro de reconstrução\n",
    "# Erro no treino\n",
    "train_pred = autoencoder.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(train_pred - X_train), axis=(1, 2))\n",
    "\n",
    "# Erro no teste\n",
    "test_pred = autoencoder.predict(X_test)\n",
    "test_mae_loss = np.mean(np.abs(test_pred - X_test), axis=(1, 2))\n",
    "\n",
    "# 5. Definir o limiar de anomalia\n",
    "# Usaremos o 95º percentil do erro de treino como nosso limiar\n",
    "# Esta é uma abordagem estatística comum.\n",
    "anomaly_threshold = np.percentile(train_mae_loss, 95)\n",
    "print(f\"\\nLimiar de anomalia definido: {anomaly_threshold:.4f}\")\n",
    "\n",
    "# Criar um DataFrame com os erros de teste para visualização\n",
    "# Os erros correspondem ao final de cada janela\n",
    "test_error_df = pd.DataFrame(\n",
    "    index=test_df.index[WINDOW_SIZE-1:],\n",
    "    data={'error': test_mae_loss}\n",
    ")\n",
    "\n",
    "# 6. Visualizar o erro de reconstrução\n",
    "plot_reconstruction_error(test_error_df, anomaly_threshold)\n",
    "\n",
    "# 7. Identificar e visualizar anomalias\n",
    "predicted_anomalies_mask = test_mae_loss > anomaly_threshold\n",
    "\n",
    "# Ajustar o tamanho dos rótulos verdadeiros para corresponder às predições\n",
    "true_anomalies_mask = test_labels.iloc[WINDOW_SIZE-1:].values.astype(bool)\n",
    "\n",
    "print(\"\\n--- Métricas de Avaliação ---\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    true_anomalies_mask, predicted_anomalies_mask, average='binary'\n",
    ")\n",
    "print(f\"Precisão: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\\n\")\n",
    "\n",
    "# Matriz de Confusão\n",
    "cm = confusion_matrix(true_anomalies_mask, predicted_anomalies_mask)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Anomalia'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.show()\n",
    "\n",
    "# Visualizar as anomalias nos dados originais\n",
    "# Normalizamos os dados de teste para a plotagem\n",
    "df_test_scaled = pd.DataFrame(test_scaled, index=test_df.index, columns=test_df.columns)\n",
    "\n",
    "# Obter os índices onde as anomalias foram detectadas\n",
    "anomaly_indices = np.where(predicted_anomalies_mask)[0]\n",
    "\n",
    "# Como as predições são por janela, precisamos mapear de volta para o timestamp original\n",
    "plot_anomalies(df_test_scaled, anomaly_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
