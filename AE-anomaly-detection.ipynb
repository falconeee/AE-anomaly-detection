{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bb3d68",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from utils.futurai_ppd import drop_transitorio_desligado\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "\n",
    "# Silenciar logs menos importantes do TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3d636",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = 'Timestamp'\n",
    "\n",
    "df_dataset = pd.read_csv('data/Depurador 762-28-006 - Cozimento.csv', sep=';', decimal='.', encoding='utf-8-sig')\n",
    "df_dataset.drop(columns=[\"762P0013.OP\", \"762F0040.OP\", \"762F0014.SP\", \"762H0336.PV\", \"762H0342.PV\", \"762N0015.SP\", \"762P0013.SP\", \"762-34-073.CR\", \"762N0015.OP\"], inplace=True, errors='ignore')\n",
    "df_dataset.dropna(inplace=True)\n",
    "df_dataset[timestamp] = pd.to_datetime(df_dataset[timestamp], format='%Y-%m-%d %H:%M:%S')\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60001a54",
   "metadata": {},
   "source": [
    "# Remove periods off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = []\n",
    "pp_var_ref_desligado = \"762-28-006.CR\"\n",
    "pp_valor_ref_desligado = 5\n",
    "pp_tempo_ref_desligado = 0\n",
    "pp_pre_corte_transitorio = 0\n",
    "pp_pos_corte_transitorio = 0\n",
    "pre_process.append(  \n",
    "{\n",
    "   \"after_cut\": pp_pos_corte_transitorio,\n",
    "   \"interval_off\": pp_tempo_ref_desligado,\n",
    "   \"limit_off\": pp_valor_ref_desligado,\n",
    "   \"pre_cut\": pp_pre_corte_transitorio,\n",
    "   \"variable_off\": pp_var_ref_desligado\n",
    "  })\n",
    "\n",
    "for pro in pre_process:\n",
    "    df_dataset,_,_ = drop_transitorio_desligado(df_dataset,pro[\"variable_off\"],pro[\"limit_off\"],pro[\"interval_off\"],timestamp,pre_corte=pro[\"pre_cut\"],pos_corte=pro[\"after_cut\"])\n",
    "print(f\"Dataset shape: {df_dataset.shape}\")\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b14288",
   "metadata": {},
   "source": [
    "# Train with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alldata = df_dataset.copy()\n",
    "eixo_tempo = df_alldata[timestamp]\n",
    "df_alldata.drop(columns=[timestamp], inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_alldata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ad8cb",
   "metadata": {},
   "source": [
    "# Split train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b38c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_train = pd.to_datetime('YYYY-mm-dd hh:mm:ss')\n",
    "end_date_train = pd.to_datetime('YYYY-mm-dd hh:mm:ss')\n",
    "\n",
    "mask = (df_dataset[timestamp] >= start_date_train) & (df_dataset[timestamp] <= end_date_train)\n",
    "df_train = df_dataset.loc[mask]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4f0e9",
   "metadata": {},
   "source": [
    "# Build autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CONSTRUÇÃO DO MODELO AUTOENCODER LSTM\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_lstm_autoencoder(input_shape):\n",
    "    \"\"\"Cria e compila um modelo de Autoencoder LSTM.\"\"\"\n",
    "    # Encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # A dimensionalidade latente é 32\n",
    "    encoded = LSTM(128, activation='relu', return_sequences=False)(inputs)\n",
    "    encoded = RepeatVector(input_shape[0])(encoded) # Repete o vetor latente para a entrada do decoder\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = LSTM(128, activation='relu', return_sequences=True)(encoded)\n",
    "    decoded = TimeDistributed(Dense(input_shape[1]))(decoded)\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mae') # Mean Absolute Error é robusto a outliers\n",
    "    \n",
    "    autoencoder.summary()\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3775388",
   "metadata": {},
   "source": [
    "# Plot and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FUNÇÕES DE PLOTAGEM E AVALIAÇÃO\n",
    "# -----------------------------------------------------------------------------\n",
    "def plot_reconstruction_error(error_df, threshold):\n",
    "    \"\"\"Plota o erro de reconstrução e o limiar de anomalia.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(error_df.index, error_df['error'], label='Erro de Reconstrução')\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label='Limiar de Anomalia')\n",
    "    plt.title('Erro de Reconstrução ao Longo do Tempo')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Erro (MAE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_anomalies(data, anomaly_indices):\n",
    "    \"\"\"Plota a série temporal original destacando as anomalias detectadas.\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for col in data.columns:\n",
    "        plt.plot(data.index, data[col], label=col, alpha=0.7)\n",
    "    \n",
    "    # Destaca as anomalias detectadas\n",
    "    anomalies = data.iloc[anomaly_indices]\n",
    "    for col in anomalies.columns:\n",
    "        plt.scatter(anomalies.index, anomalies[col], color='red', marker='x', s=50, label=f'Anomalia em {col}')\n",
    "        \n",
    "    plt.title('Detecção de Anomalias na Série Temporal Multivariada')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Valor Normalizado')\n",
    "    \n",
    "    # Lidar com legendas duplicadas\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edace870",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000310b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARÂMETROS\n",
    "WINDOW_SIZE = 24 # Usar 24 horas de dados para prever o próximo passo\n",
    "TRAIN_RATIO = 0.7 # 70% dos dados para treino (período normal)\n",
    "\n",
    "# 1. Carregar e dividir os dados\n",
    "df, is_anomaly = generate_data()\n",
    "n_train = int(len(df) * TRAIN_RATIO)\n",
    "\n",
    "train_df = df.iloc[:n_train]\n",
    "test_df = df.iloc[n_train:]\n",
    "test_labels = is_anomaly.iloc[n_train:]\n",
    "\n",
    "print(f\"Tamanho do dataset de treino: {len(train_df)}\")\n",
    "print(f\"Tamanho do dataset de teste: {len(test_df)}\")\n",
    "\n",
    "# 2. Pré-processar e criar janelas\n",
    "# Treinamos o scaler APENAS com dados de treino para evitar data leakage\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df)\n",
    "test_scaled = scaler.transform(test_df)\n",
    "\n",
    "# Janelas de treino (somente dados normais)\n",
    "X_train, _ = preprocess_and_window(train_df, WINDOW_SIZE)\n",
    "\n",
    "# Janelas de teste (inclui anomalias)\n",
    "X_test, _ = preprocess_and_window(test_df, WINDOW_SIZE)\n",
    "\n",
    "print(f\"Shape das janelas de treino: {X_train.shape}\")\n",
    "print(f\"Shape das janelas de teste: {X_test.shape}\")\n",
    "\n",
    "# 3. Criar e treinar o modelo\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "autoencoder = create_lstm_autoencoder(input_shape)\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')]\n",
    ")\n",
    "\n",
    "# 4. Calcular erro de reconstrução\n",
    "# Erro no treino\n",
    "train_pred = autoencoder.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(train_pred - X_train), axis=(1, 2))\n",
    "\n",
    "# Erro no teste\n",
    "test_pred = autoencoder.predict(X_test)\n",
    "test_mae_loss = np.mean(np.abs(test_pred - X_test), axis=(1, 2))\n",
    "\n",
    "# 5. Definir o limiar de anomalia\n",
    "# Usaremos o 95º percentil do erro de treino como nosso limiar\n",
    "# Esta é uma abordagem estatística comum.\n",
    "anomaly_threshold = np.percentile(train_mae_loss, 95)\n",
    "print(f\"\\nLimiar de anomalia definido: {anomaly_threshold:.4f}\")\n",
    "\n",
    "# Criar um DataFrame com os erros de teste para visualização\n",
    "# Os erros correspondem ao final de cada janela\n",
    "test_error_df = pd.DataFrame(\n",
    "    index=test_df.index[WINDOW_SIZE-1:],\n",
    "    data={'error': test_mae_loss}\n",
    ")\n",
    "\n",
    "# 6. Visualizar o erro de reconstrução\n",
    "plot_reconstruction_error(test_error_df, anomaly_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
