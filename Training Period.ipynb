{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats\n",
    "from scipy.stats import invgauss\n",
    "from scipy.stats import chi2\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "DIR_DATA = os.getcwd() + '/data/'\n",
    "timestamp = \"Timestamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(DIR_DATA + 'Digestor.csv', sep=';', decimal='.')\n",
    "\n",
    "df_dataset[timestamp]= pd.to_datetime(df_dataset[timestamp],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "print(df_dataset[timestamp].min())\n",
    "print(df_dataset[timestamp].max())\n",
    "\n",
    "for val in df_dataset:\n",
    "    if val != timestamp:\n",
    "        df_dataset[val] = pd.to_numeric(df_dataset[val],errors = \"coerce\")\n",
    "df_dataset = df_dataset.dropna()\n",
    "\n",
    "print(df_dataset.shape)\n",
    "\n",
    "cols = list(df_dataset.columns)\n",
    "cols.remove(timestamp)\n",
    "\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction error with PCA 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = df_dataset.copy()\n",
    "time = df_np[timestamp].to_list()\n",
    "df_np = df_np.drop(timestamp, axis=1).copy()\n",
    "array_np = df_np.to_numpy()\n",
    "\n",
    "##PCA\n",
    "# Normalizando dados\n",
    "scaler = StandardScaler()\n",
    "# Transformando para numpy array\n",
    "array_np_std = scaler.fit_transform(array_np)\n",
    "# Aplicando PCA com 95% de variância explicada\n",
    "# A quantidade de componentes principais é determinada automaticamente\n",
    "pca = PCA(0.95)\n",
    "pca.fit(array_np_std)\n",
    "pc = pca.transform(array_np_std)\n",
    "nc = pc.shape[1]\n",
    "print(\"Number of components: \", nc)\n",
    "\n",
    "reconstructed_df = pca.inverse_transform(pc)\n",
    "error = np.abs(array_np_std - reconstructed_df)\n",
    "error_sum = np.sum(error, axis=1)\n",
    "\n",
    "df_error = pd.DataFrame(error_sum, columns=[\"error-sum\"])\n",
    "df_error['error-ewm'] = df_error['error-sum'].ewm(alpha=0.01).mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=df_error['error-ewm'], mode=\"lines\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction with PCA VRE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dataset.copy()\n",
    "df = df.drop(timestamp, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "linhas_colunas = df.shape\n",
    "\n",
    "# Matriz covariança dos dados\n",
    "df_array = np.array(df.T)\n",
    "matrix_cov = np.cov(df_array)\n",
    "\n",
    "# SVD para decomposiação da matriz covariança\n",
    "coeff, s, _ = np.linalg.svd(matrix_cov)\n",
    "coeff = pd.DataFrame(coeff)\n",
    "\n",
    "# Metodo VRE - Calculo das componentes principais\n",
    "eps_pca = np.eye(linhas_colunas[1])\n",
    "vre = []\n",
    "\n",
    "for j in range(linhas_colunas[1]):\n",
    "\n",
    "    # Calculo da C1\n",
    "    residual = coeff.iloc[:, j : linhas_colunas[1]]\n",
    "    val3_c1 = residual.dot(residual.T)\n",
    "\n",
    "    val_ui = []\n",
    "    for i in range(linhas_colunas[1]):\n",
    "        eps_aux = eps_pca[:, i]\n",
    "        eps_til = val3_c1.dot(eps_aux.T)\n",
    "        aux = (eps_til.T.dot(matrix_cov).dot(eps_til)) / (\n",
    "            eps_til.T.dot(eps_til) ** 2\n",
    "        )\n",
    "\n",
    "        val_ui.append(aux)\n",
    "\n",
    "    vre.append(sum(val_ui) / (eps_aux.T.dot(matrix_cov).dot(eps_aux)))\n",
    "\n",
    "nc = vre.index(min(vre))\n",
    "\n",
    "# Calculo do PCA\n",
    "componentes_principais = coeff.iloc[:, 0 : nc]\n",
    "residual = coeff.iloc[:, nc : linhas_colunas[1]]\n",
    "aux_s = np.diag(s)\n",
    "df_s = pd.DataFrame(aux_s)\n",
    "val1 = df_s.iloc[: nc, : nc]\n",
    "val2_d = componentes_principais.dot(np.linalg.inv(val1)).dot(\n",
    "    componentes_principais.T\n",
    ")\n",
    "val3_c1 = residual.dot(residual.T)\n",
    "c2 = componentes_principais.dot(componentes_principais.T)\n",
    "\n",
    "# Componentes principais\n",
    "coeff = coeff.iloc[:, 0 : nc]\n",
    "coeff = np.array(coeff)\n",
    "df = np.array(df)\n",
    "principal_components = df.dot(coeff)\n",
    "# principal_components = pd.DataFrame(principal_components)\n",
    "\n",
    "print(\"Number of components: \", nc)\n",
    "\n",
    "reconstructed_df = principal_components.dot(coeff.T)\n",
    "\n",
    "error = np.abs(df - reconstructed_df)\n",
    "error_sum = np.sum(error, axis=1)\n",
    "\n",
    "df_error = pd.DataFrame(error_sum, columns=[\"error-sum\"])\n",
    "df_error['error-ewm'] = df_error['error-sum'].ewm(alpha=0.01).mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=df_error['error-ewm'], mode=\"lines\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction with AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === Pré-processamento ===\n",
    "df = df_dataset.copy()\n",
    "time = df[timestamp].to_list()\n",
    "df = df.drop(timestamp, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# === Definindo Autoencoder ===\n",
    "input_dim = df_scaled.shape[1]\n",
    "encoding_dim = min(20, input_dim // 2)  # Ajustável\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# === Treinamento do Autoencoder ===\n",
    "autoencoder.fit(df_scaled, df_scaled,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                verbose=1)\n",
    "\n",
    "# === Reconstrução e erro ===\n",
    "reconstructed = autoencoder.predict(df_scaled)\n",
    "reconstruction_error = np.mean((df_scaled - reconstructed) ** 2, axis=1)\n",
    "\n",
    "# === Cálculo do erro com suavização ===\n",
    "df_error = pd.DataFrame(reconstruction_error, columns=[\"error-mse\"])\n",
    "df_error['error-ewm'] = df_error['error-mse'].ewm(alpha=0.01).mean()\n",
    "\n",
    "# === Plotando ===\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=df_error['error-ewm'], mode=\"lines\", name=\"Erro Reconstrução EWM\"))\n",
    "fig.update_layout(title=\"Erro de Reconstrução com Autoencoder\", xaxis_title=\"Tempo\", yaxis_title=\"Erro\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual select training period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_training_period(df_dataset, timestamp):\n",
    "    df_np = df_dataset.copy()\n",
    "    time = df_np[timestamp].to_list()\n",
    "    df_np = df_np.drop(timestamp, axis=1).copy()\n",
    "    array_np = df_np.to_numpy()\n",
    "\n",
    "    ##PCA\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training set only.\n",
    "    array_np_std = scaler.fit_transform(array_np)\n",
    "    cov = np.cov(array_np_std.T)\n",
    "    u, s, vh = np.linalg.svd(cov)\n",
    "    pca = PCA(0.95)\n",
    "    pca.fit(array_np_std)\n",
    "    pc = pca.transform(array_np_std)\n",
    "    nc = pc.shape[1]\n",
    "    print(\"Number of components: \", nc)\n",
    "    s_diag = np.diag(s)\n",
    "    s_pcs = s_diag[:nc, :nc]\n",
    "\n",
    "    ##T2\n",
    "    t2 = []\n",
    "    for i in range(pc.shape[0]):\n",
    "        termo1 = pc[i]\n",
    "        termo2 = np.linalg.inv(s_pcs)\n",
    "        termo3 = pc[i].T\n",
    "\n",
    "        t2.append(termo1.dot(termo2).dot(termo3))\n",
    "    M = pc.shape[1]\n",
    "    N = pc.shape[0]\n",
    "    F = scipy.stats.f.ppf(0.95, M, N - M)\n",
    "    t2_lim = (M * (N - 1) / (N - M)) * F\n",
    "\n",
    "    ##SPE\n",
    "    spe = []\n",
    "    for i in range(pc.shape[0]):\n",
    "        rs = array_np_std[i].dot(u[:, nc - 1 :])\n",
    "        termo1 = rs.T\n",
    "        termo2 = rs\n",
    "        spe.append(termo1.dot(termo2))\n",
    "    teta1 = (s_diag[nc - 1 :]).sum()\n",
    "    teta2 = (s_diag[nc - 1 :] ** 2).sum()\n",
    "    teta3 = (s_diag[nc:-1, :] ** 3).sum()\n",
    "    h0 = 1 - (2 * teta1 * teta3) / (3 * teta2**2)\n",
    "    mu = 0.145462645553\n",
    "    vals = invgauss.ppf([0, 0.999], mu)\n",
    "    ca = invgauss.cdf(vals, mu)[1]\n",
    "    spe_lim = teta1 * (\n",
    "        (h0 * ca * np.sqrt(2 * teta2) / teta1)\n",
    "        + 1\n",
    "        + (teta2 * h0 * (h0 - 1)) / (teta1**2)\n",
    "    ) ** (1 / h0)\n",
    "\n",
    "    ##PHI\n",
    "    phi = []\n",
    "    for i in range(pc.shape[0]):\n",
    "        phi.append((spe[i] / spe_lim) + (t2[i] / t2_lim))\n",
    "    gphi = ((nc / t2_lim*2) + (teta2 / spe_lim*2)) / (\n",
    "        (nc / t2_lim) + (teta1 / spe_lim)\n",
    "    )\n",
    "    hphi = ((nc / t2_lim) + (teta1 / spe_lim)) ** 2 / (\n",
    "        (nc / t2_lim*2) + (teta2 / spe_lim*2)\n",
    "    )\n",
    "    chi2.ppf(0.975, df=2)\n",
    "    phi_lim = gphi * chi2.ppf(0.99, hphi)\n",
    "    df_t2 = pd.DataFrame(\n",
    "        {\n",
    "            \"time\": time,\n",
    "            \"t2\": t2,\n",
    "            \"spe\": spe,\n",
    "            \"phi\": phi,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_t2[\"t2_lim\"] = t2_lim\n",
    "    df_t2[\"spe_lim\"] = spe_lim\n",
    "    df_t2[\"phi_lim\"] = phi_lim\n",
    "\n",
    "    df_t2[\"t2\"] = df_t2[\"t2\"].ewm(alpha=0.01).mean()\n",
    "    df_t2[\"spe\"] = df_t2[\"spe\"].ewm(alpha=0.01).mean()\n",
    "    df_t2[\"phi\"] = df_t2[\"phi\"].ewm(alpha=0.01).mean()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df_t2[\"time\"], y=df_t2[\"phi\"], mode=\"lines\"))\n",
    "    fig.add_trace(go.Scatter(x=df_t2[\"time\"], y=df_t2[\"phi_lim\"], mode=\"lines\"))\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = select_training_period(df_dataset, timestamp)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FuturaiML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import f, norm\n",
    "from scipy.stats.distributions import chi2\n",
    "import math\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "\n",
    "class FuturaiML:\n",
    "    \"\"\"\n",
    "    Classe para geração do modelo futurai\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nc: int = 0, gain: int = 1):\n",
    "        self.gain = gain\n",
    "        self.nc = nc\n",
    "\n",
    "    def fit(self, x_train: pd.DataFrame):\n",
    "        \"\"\"Função para treinamento do modelo\n",
    "        :param - data - dataframe com a dataset de treinamento\n",
    "        :return none\n",
    "        \"\"\"\n",
    "\n",
    "        # Faz o scalling da base\n",
    "        df = (x_train - x_train.mean()) / x_train.std()\n",
    "        self.media = x_train.mean()\n",
    "        self.std = x_train.std()\n",
    "\n",
    "        # aplicação do PCA para redução da quantidade de features do dataset\n",
    "        linhas_colunas = df.shape\n",
    "\n",
    "        # Matriz covariança dos dados\n",
    "        df_array = np.array(df.T)\n",
    "        matrix_cov = np.cov(df_array)\n",
    "\n",
    "        # SVD para decomposiação da matriz covariança\n",
    "        coeff, s, _ = np.linalg.svd(matrix_cov)\n",
    "        coeff = pd.DataFrame(coeff)\n",
    "\n",
    "        # Metodo VRE - Calculo das componentes principais\n",
    "        if self.nc == 0:\n",
    "\n",
    "            eps_pca = np.eye(linhas_colunas[1])\n",
    "            vre = []\n",
    "\n",
    "            for j in range(linhas_colunas[1]):\n",
    "\n",
    "                # Calculo da C1\n",
    "                residual = coeff.iloc[:, j : linhas_colunas[1]]\n",
    "                val3_c1 = residual.dot(residual.T)\n",
    "\n",
    "                val_ui = []\n",
    "                for i in range(linhas_colunas[1]):\n",
    "                    eps_aux = eps_pca[:, i]\n",
    "                    eps_til = val3_c1.dot(eps_aux.T)\n",
    "                    aux = (eps_til.T.dot(matrix_cov).dot(eps_til)) / (\n",
    "                        eps_til.T.dot(eps_til) ** 2\n",
    "                    )\n",
    "\n",
    "                    val_ui.append(aux)\n",
    "\n",
    "                vre.append(sum(val_ui) / (eps_aux.T.dot(matrix_cov).dot(eps_aux)))\n",
    "\n",
    "            self.nc = vre.index(min(vre))\n",
    "\n",
    "        # Calculo do PCA\n",
    "        componentes_principais = coeff.iloc[:, 0 : self.nc]\n",
    "        residual = coeff.iloc[:, self.nc : linhas_colunas[1]]\n",
    "        aux_s = np.diag(s)\n",
    "        df_s = pd.DataFrame(aux_s)\n",
    "        val1 = df_s.iloc[: self.nc, : self.nc]\n",
    "        val2_d = componentes_principais.dot(np.linalg.inv(val1)).dot(\n",
    "            componentes_principais.T\n",
    "        )\n",
    "        val3_c1 = residual.dot(residual.T)\n",
    "        self.c2 = componentes_principais.dot(componentes_principais.T)\n",
    "\n",
    "        # Componentes principais\n",
    "        coeff = coeff.iloc[:, 0 : self.nc]\n",
    "        coeff = np.array(coeff)\n",
    "        df = np.array(df)\n",
    "        principal_components = df.dot(coeff)\n",
    "        principal_components = pd.DataFrame(principal_components)\n",
    "\n",
    "        self.val_d = val2_d\n",
    "        self.val_c1 = val3_c1\n",
    "\n",
    "        # Gera calculo dos limiares\n",
    "        base_dados = principal_components\n",
    "        a = self.nc\n",
    "        ds = s\n",
    "\n",
    "        alfa = 0.99\n",
    "        n = base_dados.shape\n",
    "        n = n[0]\n",
    "\n",
    "        # limiar da t2\n",
    "        t2_lim = (a * (n - 1) * (n + 1) / (n * (n - a))) * f.ppf(alfa, a, n - a)\n",
    "\n",
    "        # limiar da Q\n",
    "        teta1 = sum(ds[a:])\n",
    "        teta2 = sum(ds[a:] ** 2)\n",
    "        teta3 = sum(ds[a:] ** 3)\n",
    "\n",
    "        h0 = 1 - (2 * teta1 * teta3) / (3 * (teta2 ** 2))\n",
    "        ca = norm.ppf(alfa, 0, 1)\n",
    "        q_lim = teta1 * (\n",
    "            (h0 * ca * (math.sqrt(2 * teta2)) / teta1)\n",
    "            + 1\n",
    "            + (teta2 * h0 * (h0 - 1)) / (teta1 ** 2)\n",
    "        ) ** (1 / h0)\n",
    "\n",
    "        # limiar phi\n",
    "        gphi = ((a / t2_lim ** 2) + (teta2 / q_lim ** 2)) / (\n",
    "            (a / t2_lim) + (teta1 / q_lim)\n",
    "        )\n",
    "        hphi = ((a / t2_lim) + (teta1 / q_lim)) ** 2 / (\n",
    "            (a / t2_lim ** 2) + (teta2 / q_lim ** 2)\n",
    "        )\n",
    "\n",
    "        phi_lim = gphi * chi2.ppf(alfa, hphi)\n",
    "\n",
    "        self.t2_lim = t2_lim\n",
    "        self.q_lim = q_lim\n",
    "        self.phi_lim = phi_lim * self.gain\n",
    "\n",
    "    def predict(self, x_test, eixo_x, points=2) -> Dict:\n",
    "        \"\"\"Realiza a predição da base de dados\n",
    "        :param - x_test: Base de dados para Predição\n",
    "        :return - phi_matrix: uma matriz\"\"\"\n",
    "\n",
    "        base = (x_test - self.media) / self.std\n",
    "\n",
    "        abase = np.array(base)\n",
    "        aval_d = self.val_d\n",
    "        aval_c1 = self.val_c1\n",
    "        t2_lim = self.t2_lim\n",
    "        q_lim = self.q_lim\n",
    "\n",
    "        # Estatistica Phi\n",
    "        phi = []\n",
    "        phi_matrix = (aval_d / t2_lim) + (aval_c1 / q_lim)\n",
    "\n",
    "        for i in range(len(base)):\n",
    "            phi.append(float((abase[i, :].T).dot(phi_matrix).dot(abase[i, :])))\n",
    "\n",
    "        # Filtro\n",
    "        dataset = pd.DataFrame([list(eixo_x), phi], index=[\"TIMESTAMP\", \"PHI\"]).T\n",
    "        dataset[\"TIMESTAMP\"] = pd.to_datetime(\n",
    "            dataset[\"TIMESTAMP\"], format=\"%Y/%m/%d %H:%M:%S\"\n",
    "        )\n",
    "        df_aux = dataset.copy()\n",
    "\n",
    "        df_aux[\"status\"] = 1\n",
    "        ############################   Subida  ########################################\n",
    "        ## Essa parte do código serve para pegar picos onde o motor volta a \"funcionar\"\n",
    "        ## por menos de uma hora, ou seja, ele estava desligado, deu um pique de menos\n",
    "        ## de uma hora e voltou a ficar desligado\n",
    "        ###############################################################################\n",
    "        data_aux = df_aux[\"TIMESTAMP\"].min()  # Primeira data do dataframe\n",
    "        while True:\n",
    "\n",
    "            # Pega a data da primeira amostra com o valor abaixo do limite\n",
    "            df_amostra = df_aux[\n",
    "                (df_aux[\"PHI\"] > self.phi_lim) & (df_aux[\"TIMESTAMP\"] >= data_aux)\n",
    "            ]\n",
    "\n",
    "            if not df_amostra.empty:\n",
    "                data_min = df_amostra[\"TIMESTAMP\"].min()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # Pega a primeira data da amostra acima do valor limite depois da amostra acima\n",
    "            df_amostra = df_aux[\n",
    "                (df_aux[\"PHI\"] <= self.phi_lim) & (df_aux[\"TIMESTAMP\"] > data_min)\n",
    "            ]\n",
    "\n",
    "            if not df_amostra.empty:\n",
    "                data_aux = df_amostra[\"TIMESTAMP\"].min()\n",
    "\n",
    "                mask = (df_aux[\"TIMESTAMP\"] >= data_min) & (\n",
    "                    df_aux[\"TIMESTAMP\"] < data_aux\n",
    "                )\n",
    "\n",
    "                df_amostra = df_aux.loc[mask]\n",
    "\n",
    "                if len(df_amostra) <= points:\n",
    "                    df_aux[\"status\"].loc[mask] = 0\n",
    "\n",
    "            else:\n",
    "                data_aux = df_aux[\"TIMESTAMP\"].max()\n",
    "\n",
    "                mask = (df_aux[\"TIMESTAMP\"] >= data_min) & (\n",
    "                    df_aux[\"TIMESTAMP\"] <= data_aux\n",
    "                )\n",
    "\n",
    "                df_amostra = df_aux.loc[mask]\n",
    "\n",
    "                if len(df_amostra) <= points:\n",
    "                    df_aux[\"status\"].loc[mask] = 0\n",
    "\n",
    "                break\n",
    "\n",
    "        dataset.drop(df_aux[df_aux[\"status\"] == 0].index, inplace=True)\n",
    "\n",
    "        phi = list(dataset[\"PHI\"])\n",
    "        eixo_x = list(dataset[\"TIMESTAMP\"].dt.strftime(\"%Y-%m-%d %X\"))\n",
    "\n",
    "        predicao = {\"matrix\": phi_matrix, \"phi\": phi, \"timestamp\": eixo_x}\n",
    "\n",
    "        return predicao\n",
    "\n",
    "    def contribuition(self, df, phi, df_sistema, eixo_x):\n",
    "        \"\"\"Função para gerar o grafico os de Contribuição e\n",
    "        também lista das varaiveis que mais influênciaram\"\"\"\n",
    "\n",
    "        df = (df - self.media) / self.std\n",
    "        linhas_colunas = df.shape\n",
    "\n",
    "        seq_aux = list(range(len(df)))\n",
    "\n",
    "        n_fast_list = pd.DataFrame([seq_aux])\n",
    "        n_fast_list = n_fast_list.T\n",
    "        n_fast_list.columns = [\"SEQ\"]\n",
    "        rci = []\n",
    "\n",
    "        # Geração da matriz de contribuição\n",
    "        for x in list(range(linhas_colunas[1])):\n",
    "\n",
    "            # Definindo epsilon - só entram as variáveis em falta\n",
    "            eps = np.eye(linhas_colunas[1])\n",
    "            eps = eps[:, x]\n",
    "            eps = pd.DataFrame(eps)\n",
    "\n",
    "            # Definindo Trci - diagonal matrix which the elements are one for the faulty variables\n",
    "            k = [0] * linhas_colunas[1]\n",
    "            k[x] = 1\n",
    "            trci = np.diag(k)\n",
    "\n",
    "            # equação 13 - reconstrução das variáveis em falta\n",
    "            termo_1 = -np.linalg.inv(eps.T.dot(phi).dot(eps))\n",
    "            termo_2 = eps.T\n",
    "            termo_3 = phi\n",
    "            termo_4 = pd.DataFrame(np.eye(linhas_colunas[1]) - trci)\n",
    "            termo_5 = df.T\n",
    "\n",
    "            n_fast = termo_1.dot(termo_2).dot(termo_3).dot(termo_4).dot(termo_5)\n",
    "            n_fast = n_fast.T\n",
    "            n_fast = pd.DataFrame(n_fast)\n",
    "\n",
    "            # Equação 14 do RCI\n",
    "            termo_3 = (pd.DataFrame(df.iloc[:, x]).values) - n_fast.values\n",
    "            termo_1 = termo_3.T\n",
    "            termo_2 = eps.T.dot(phi).dot(eps)\n",
    "            termo_2 = float(termo_2.values)\n",
    "\n",
    "            # RCI contem o score de importancia de cada variavel no momento da falha\n",
    "            rci.append(float((termo_1 * termo_2).dot(termo_3)))\n",
    "\n",
    "            n_fast = pd.DataFrame(n_fast)\n",
    "            n_fast_list = pd.concat([n_fast_list, n_fast], axis=1)\n",
    "\n",
    "        n_fast_list = n_fast_list.drop([\"SEQ\"], axis=1)\n",
    "        n_fast_list.columns = df.columns\n",
    "\n",
    "        termo_1 = df - n_fast_list\n",
    "        termo_2 = eps.T.dot(phi).dot(eps)\n",
    "        termo_3 = termo_2 ** 0.5\n",
    "\n",
    "        circi = termo_1 * float(termo_3.values)\n",
    "        circi = circi ** 2\n",
    "\n",
    "        df_sistema[\"score\"] = 0\n",
    "        # Monta um dataframe de forma decrescente das varaiveis conforme seu score\n",
    "        df_rci = pd.DataFrame({\"score\": rci, \"variavel\": df.columns})\n",
    "        df_rci = df_rci.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "        for _, row in df_rci.iterrows():\n",
    "            tag = row[\"variavel\"]\n",
    "            val = row[\"score\"]\n",
    "            idx = df_sistema[df_sistema[\"VARIAVEL\"] == tag].index\n",
    "\n",
    "            df_sistema.loc[idx, \"score\"] = val\n",
    "\n",
    "        df_score_dec = df_sistema.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "        # Recria a phi tirando um a um as varaiveis que mais influenciam até o phi ficar abaixo do limiar\n",
    "        val_contr = []\n",
    "        qtd_aux = 1\n",
    "\n",
    "        for i, row in df_rci.iterrows():\n",
    "\n",
    "            val_contr.append(i)\n",
    "\n",
    "            eps = np.eye(linhas_colunas[1])\n",
    "            eps = eps[:, val_contr]\n",
    "            eps = pd.DataFrame(eps)\n",
    "\n",
    "            # Definindo Trci - diagonal matrix which the elements are one for the faulty variables\n",
    "            k = [0] * linhas_colunas[1]\n",
    "            for x in val_contr:\n",
    "                k[x] = 1\n",
    "            trci = np.diag(k)\n",
    "\n",
    "            # equação 13 - reconstrução das variáveis em falta\n",
    "            termo_1 = -np.linalg.inv(eps.T.dot(phi).dot(eps))\n",
    "            termo_2 = eps.T\n",
    "            termo_3 = phi\n",
    "            termo_4 = pd.DataFrame(np.eye(linhas_colunas[1]) - trci)\n",
    "            termo_5 = df.T\n",
    "\n",
    "            n_fast = termo_1.dot(termo_2).dot(termo_3).dot(termo_4).dot(termo_5)\n",
    "            n_fast = n_fast.T\n",
    "\n",
    "            phiast = []\n",
    "            for x in list(range(linhas_colunas[0])):\n",
    "\n",
    "                termo_1 = df.iloc[x, :].values\n",
    "                termo_2 = np.eye(linhas_colunas[1]) - trci\n",
    "                termo_3 = phi\n",
    "                termo_4 = termo_2\n",
    "                termo_5 = df.iloc[x, :].T.values\n",
    "                termo_6 = n_fast[x, :]\n",
    "                termo_7 = eps.T.dot(phi).dot(eps)\n",
    "                termo_8 = termo_6.T\n",
    "\n",
    "                phiast.append(\n",
    "                    float(\n",
    "                        (termo_1.dot(termo_2).dot(termo_3).dot(termo_4).dot(termo_5))\n",
    "                        - (termo_6.dot(termo_7).dot(termo_8))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if max(phiast) < self.phi_lim:\n",
    "                break\n",
    "\n",
    "            # Quantidade de varaiveis que mais influenciaram\n",
    "            qtd_aux = qtd_aux + 1\n",
    "\n",
    "        # Separa as varaiveis que mais influenciam das restantes\n",
    "        df_score_prin = df_score_dec.iloc[0:qtd_aux].copy()\n",
    "        df_score_prin.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        df_score_res = df_score_dec.iloc[qtd_aux:].copy()\n",
    "\n",
    "        ######## Geração do grafico hierarquico conforme local ########\n",
    "        soma = df_score_prin[\"score\"].sum()\n",
    "        df_score_prin[\"%\"] = df_score_prin.score.apply(\n",
    "            lambda x: round((x / soma * 100), 5)\n",
    "        )\n",
    "\n",
    "        soma = df_score_dec[\"score\"].sum()\n",
    "        df_score_dec[\"%\"] = df_score_dec.score.apply(\n",
    "            lambda x: round((x / soma * 100), 5)\n",
    "        )\n",
    "\n",
    "        # para geração do grafico de contribuição as variaiveis\n",
    "        # que menos influenciaram são zeradas para não poluir o grafico\n",
    "        df_contribuicao = circi.copy()\n",
    "        for x in df_score_res[\"VARIAVEL\"]:\n",
    "            df_contribuicao.loc[:, x] = 0\n",
    "\n",
    "        ######## Geração do grafico de contribuição ########\n",
    "        df_contribuicao = df_contribuicao.join(\n",
    "            pd.Series(list(eixo_x)).rename(\"timestamp\"), how=\"right\"\n",
    "        )\n",
    "\n",
    "        df_contribuicao = df_contribuicao.to_json(orient=\"columns\")\n",
    "        df_score_prin = df_score_prin.to_json(orient=\"columns\")\n",
    "        df_score_dec = df_score_dec.to_json(orient=\"columns\")\n",
    "\n",
    "        return (\n",
    "            json.loads(df_score_prin),\n",
    "            json.loads(df_contribuicao),\n",
    "            json.loads(df_score_dec),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
