{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942b8b3f",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats\n",
    "from scipy.stats import invgauss\n",
    "from scipy.stats import chi2\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "DIR_DATA = os.getcwd() + '/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ccc85",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = \"Timestamp\"\n",
    "\n",
    "df_dataset = pd.read_csv(DIR_DATA + 'Digestor.csv', sep=';', decimal='.')\n",
    "\n",
    "df_dataset[timestamp]= pd.to_datetime(df_dataset[timestamp],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "print(df_dataset[timestamp].min())\n",
    "print(df_dataset[timestamp].max())\n",
    "\n",
    "for val in df_dataset:\n",
    "    if val != timestamp:\n",
    "        df_dataset[val] = pd.to_numeric(df_dataset[val],errors = \"coerce\")\n",
    "df_dataset = df_dataset.dropna()\n",
    "\n",
    "print(df_dataset.shape)\n",
    "\n",
    "cols = list(df_dataset.columns)\n",
    "cols.remove(timestamp)\n",
    "\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cae962",
   "metadata": {},
   "source": [
    "# Remove offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f61e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_transitorio_desligado(\n",
    "    dataset,\n",
    "    variavel,\n",
    "    limite,\n",
    "    intervalo,\n",
    "    timestamp,\n",
    "    pre_corte=0,\n",
    "    pos_corte=0,\n",
    "    pp_residual=0,\n",
    "):\n",
    "    final = False\n",
    "    df_aux = dataset.loc[:, [variavel, timestamp]].copy()\n",
    "    df_aux[\"status\"] = 1  # Ligado\n",
    "    ############################   Descida  ########################################\n",
    "    ## Se uma amostra está abaixo do limite e permance nesse nivel por\n",
    "    ## por um intervalo, o status desse periodo é setado como 0, o que indicada\n",
    "    ## que o motor está desligado\n",
    "    ###############################################################################\n",
    "    data_aux = df_aux[timestamp].min()  # Primeira data do dataframe\n",
    "    list_periods = []\n",
    "    while True:\n",
    "        # Pega a data da primeira amostra com o valor abaixo do limite\n",
    "        df_amostra = df_aux[\n",
    "            (df_aux[variavel] <= limite) & (df_aux[timestamp] >= data_aux)\n",
    "        ]\n",
    "\n",
    "        if not df_amostra.empty:\n",
    "            data_min = df_amostra[timestamp].min()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        # Pega a primeira data da amostra acima do valor limite depois da amostra acima\n",
    "        df_amostra = df_aux[\n",
    "            (df_aux[variavel] > limite) & (df_aux[timestamp] > data_min)\n",
    "        ]\n",
    "\n",
    "        if not df_amostra.empty:\n",
    "            data_aux = df_amostra[timestamp].min()\n",
    "        else:\n",
    "            data_aux = df_aux[timestamp].max()\n",
    "            final = True\n",
    "\n",
    "        # Tira a diferença entre as duas amostras,\n",
    "        dif_date = (data_aux - data_min).total_seconds()\n",
    "        dif_date = dif_date/60\n",
    "\n",
    "        # Caso o valor da diferença seja maior que >= 3600s (1 hora) o motor está desligado\n",
    "        if dif_date >= intervalo:\n",
    "            list_periods.append(\n",
    "                {\"date_ini\": data_min, \"date_end\": data_aux, \"type\": \"desligado\"}\n",
    "            )\n",
    "            mask = (df_aux[timestamp] >= data_min) & (df_aux[timestamp] <= data_aux)\n",
    "            df_aux[\"status\"].loc[mask] = 0  # Status Desligado\n",
    "\n",
    "            if pre_corte != 0:\n",
    "                date_ini_pre_corte = data_min - timedelta(minutes=pre_corte)\n",
    "                date_end_pre_corte = data_min\n",
    "\n",
    "                mask_pre_corte = (df_aux[timestamp] >= date_ini_pre_corte) & (\n",
    "                    df_aux[timestamp] <= date_end_pre_corte\n",
    "                )\n",
    "                df_aux[\"status\"].loc[mask_pre_corte] = 0  # Status Desligado\n",
    "                list_periods.append(\n",
    "                    {\n",
    "                        \"date_ini\": date_ini_pre_corte,\n",
    "                        \"date_end\": date_end_pre_corte,\n",
    "                        \"type\": \"transitorio\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if pos_corte != 0:\n",
    "                date_ini_pos_corte = data_aux\n",
    "                date_end_pos_corte = data_aux + timedelta(minutes=pos_corte)\n",
    "\n",
    "                mask_pos_corte = (df_aux[timestamp] >= date_ini_pos_corte) & (\n",
    "                    df_aux[timestamp] <= date_end_pos_corte\n",
    "                )\n",
    "                df_aux[\"status\"].loc[mask_pos_corte] = 0  # Status Desligado\n",
    "                list_periods.append(\n",
    "                    {\n",
    "                        \"date_ini\": date_ini_pos_corte,\n",
    "                        \"date_end\": date_end_pos_corte,\n",
    "                        \"type\": \"transitorio\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if final:\n",
    "            break\n",
    "\n",
    "    ############################   Subida  ########################################\n",
    "    ## Essa parte do código serve para pegar picos onde o motor volta a \"funcionar\"\n",
    "    ## por menos do intervalo, ou seja, ele estava desligado, deu um pique de menos\n",
    "    ## de uma hora e voltou a ficar desligado\n",
    "    ###############################################################################\n",
    "    data_aux = df_aux[timestamp].min()\n",
    "    while True:\n",
    "        # Pega a primeira data da amostra com status ligado\n",
    "        df_amostra = df_aux[(df_aux[\"status\"] == 1) & (df_aux[timestamp] >= data_aux)]\n",
    "\n",
    "        if not df_amostra.empty:\n",
    "            data_min = df_amostra[timestamp].min()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        # Pega a amostra com o status deligado após a data acima\n",
    "        df_amostra = df_aux[(df_aux[\"status\"] == 0) & (df_aux[timestamp] > data_min)]\n",
    "\n",
    "        if not df_amostra.empty:\n",
    "            data_aux = df_amostra[timestamp].min()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        # Tira a diferença entre as duas amostras,\n",
    "        dif_date = (data_aux - data_min).total_seconds()\n",
    "\n",
    "        # Caso o valor da diferença seja maior que < 3600s (1 hora) o motor está desligado\n",
    "        if dif_date < intervalo:\n",
    "            list_periods.append(\n",
    "                {\"date_ini\": data_min, \"date_end\": data_aux, \"type\": \"desligado\"}\n",
    "            )\n",
    "            mask = (df_aux[timestamp] >= data_min) & (df_aux[timestamp] <= data_aux)\n",
    "            df_aux[\"status\"].loc[mask] = 0\n",
    "\n",
    "    df_aux[\"status\"].iloc[:pp_residual] = 0\n",
    "    df_return = dataset.copy()\n",
    "    df_return.drop(df_aux[df_aux[\"status\"] == 0].index, inplace=True)\n",
    "    return df_return, df_aux, list_periods\n",
    "\n",
    "pre_process = []\n",
    "pp_var_ref_desligado = \"211S001M.TT\"\n",
    "pp_valor_ref_desligado = 5\n",
    "pp_tempo_ref_desligado = 0\n",
    "pp_pre_corte_transitorio = 60\n",
    "pp_pos_corte_transitorio = 60\n",
    "pre_process.append(  \n",
    "{\n",
    "   \"after_cut\": pp_pos_corte_transitorio,\n",
    "   \"interval_off\": pp_tempo_ref_desligado,\n",
    "   \"limit_off\": pp_valor_ref_desligado,\n",
    "   \"pre_cut\": pp_pre_corte_transitorio,\n",
    "   \"variable_off\": pp_var_ref_desligado\n",
    "  })\n",
    "\n",
    "for pro in pre_process:\n",
    "    df_dataset_ppd,_,_ = drop_transitorio_desligado(df_dataset,pro[\"variable_off\"],pro[\"limit_off\"],pro[\"interval_off\"],timestamp,pre_corte=pro[\"pre_cut\"],pos_corte=pro[\"after_cut\"])\n",
    "\n",
    "df_dataset_ppd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdc772",
   "metadata": {},
   "source": [
    "# Select training periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad088d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pré-processamento ===\n",
    "df = df_train.copy()\n",
    "time = df[timestamp].to_list()\n",
    "df = df.drop(timestamp, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# === Definindo Autoencoder ===\n",
    "input_dim = df_scaled.shape[1]\n",
    "encoding_dim = min(20, input_dim // 2)  # Ajustável\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# === Treinamento do Autoencoder ===\n",
    "autoencoder.fit(df_scaled, df_scaled,\n",
    "                epochs=10,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                verbose=1)\n",
    "\n",
    "# === Reconstrução e erro ===\n",
    "reconstructed = autoencoder.predict(df_scaled)\n",
    "reconstruction_error = np.mean((df_scaled - reconstructed) ** 2, axis=1)\n",
    "\n",
    "# === Cálculo do erro com suavização ===\n",
    "df_error = pd.DataFrame(reconstruction_error, columns=[\"error-mse\"])\n",
    "df_error['error-ewm'] = df_error['error-mse'].ewm(alpha=0.01).mean()\n",
    "\n",
    "# === Plotando ===\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=df_error['error-ewm'], mode=\"lines\", name=\"Erro Reconstrução EWM\"))\n",
    "fig.update_layout(title=\"Erro de Reconstrução com Autoencoder\", xaxis_title=\"Tempo\", yaxis_title=\"Erro\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e86eb7",
   "metadata": {},
   "source": [
    "# Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stable_autoencoder(X_stable, encoding_dim=8, epochs=100, batch_size=32, verbose=1):\n",
    "    input_dim = X_stable.shape[1]\n",
    "\n",
    "    # Definição do modelo\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoding_dim * 2, activation='relu')(input_layer)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    # Treinamento\n",
    "    autoencoder.fit(X_stable, X_stable,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=verbose)\n",
    "\n",
    "    # Modelo encoder para projeção no espaço latente\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "start_date_train = pd.to_datetime('2023-09-02 00:00:00')\n",
    "end_date_train = pd.to_datetime('2023-09-12 00:00:00')\n",
    "\n",
    "mask = (df_dataset[timestamp] >= start_date_train) & (df_dataset[timestamp] <= end_date_train)\n",
    "df_train = df_dataset.loc[mask].drop(columns=[timestamp])\n",
    "\n",
    "# Escala os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train)\n",
    "\n",
    "# Treina o autoencoder\n",
    "autoencoder, encoder = train_stable_autoencoder(X_train, epochs=10, encoding_dim=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6610b2",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_test = pd.to_datetime('2023-09-01 00:00:00')\n",
    "end_date_test = pd.to_datetime('2025-09-12 00:00:00')\n",
    "\n",
    "mask = (df_dataset[timestamp] >= start_date_test) & (df_dataset[timestamp] <= end_date_test)\n",
    "df_test = df_dataset.loc[mask]\n",
    "eixo_teste = df_test[timestamp].to_list()\n",
    "df_test = df_test.drop(columns=[timestamp])\n",
    "\n",
    "# Pré-processar novos dados com o mesmo scaler usado nos dados de treino\n",
    "X_test = scaler.transform(df_test)\n",
    "\n",
    "# Projeção no espaço latente\n",
    "X_latent = encoder.predict(X_test)\n",
    "\n",
    "# Reconstrução\n",
    "X_reconstructed = autoencoder.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1ce07",
   "metadata": {},
   "source": [
    "# Calculate reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error = np.mean((X_test - X_reconstructed)**2, axis=1)\n",
    "\n",
    "df_result = pd.DataFrame({\n",
    "    \"timestamp\": eixo_teste,\n",
    "    \"reconstruction_error\": reconstruction_error\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=eixo_teste, y=reconstruction_error, mode=\"lines\", name=\"Erro Reconstrução\"))\n",
    "fig.update_layout(title=\"Erro de Reconstrução com Autoencoder\", xaxis_title=\"Tempo\", yaxis_title=\"Erro\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12561c",
   "metadata": {},
   "source": [
    "# Detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53065d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_test = pd.to_datetime('2023-12-28 00:00:00')\n",
    "end_date_test = pd.to_datetime('2024-01-01 00:00:00')\n",
    "\n",
    "mask = (df_dataset[timestamp] >= start_date_test) & (df_dataset[timestamp] <= end_date_test)\n",
    "df_anom = df_dataset.loc[mask]\n",
    "eixo_anom = df_anom[timestamp].to_list()\n",
    "df_anom = df_anom.drop(columns=[timestamp])\n",
    "\n",
    "X_anom = scaler.transform(df_anom)\n",
    "\n",
    "# Projeção no espaço latente\n",
    "X_anom_latent = encoder.predict(X_anom)\n",
    "\n",
    "# Reconstrução\n",
    "X_anom_reconstructed = autoencoder.predict(X_anom)\n",
    "\n",
    "\n",
    "def get_top_error_variables(original_data, reconstructed_data, feature_names, index, top_n=10):\n",
    "    \"\"\"\n",
    "    Retorna as variáveis com maior erro de reconstrução para uma linha específica.\n",
    "\n",
    "    Parâmetros:\n",
    "    - original_data: array com os dados originais escalados\n",
    "    - reconstructed_data: array com os dados reconstruídos pelo autoencoder\n",
    "    - feature_names: lista com o nome das variáveis\n",
    "    - index: índice da linha (int) que será avaliada\n",
    "    - top_n: número de variáveis com maior erro a retornar\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame com variáveis ordenadas pelo erro de reconstrução\n",
    "    \"\"\"\n",
    "    erro_linha = np.abs(original_data[index] - reconstructed_data[index])\n",
    "\n",
    "    df_erro = pd.DataFrame({\n",
    "        'variavel': feature_names,\n",
    "        'erro_abs': erro_linha\n",
    "    }).sort_values(by='erro_abs', ascending=False)\n",
    "\n",
    "    return df_erro.head(top_n)\n",
    "\n",
    "# Obter variáveis com maior erro para a linha 10 (exemplo de anomalia)\n",
    "top_vars = get_top_error_variables(X_anom, X_anom_reconstructed, cols, index=10, top_n=10)\n",
    "\n",
    "print(top_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Um modelo wrapper para explicar a reconstrução do autoencoder\n",
    "# explainer = shap.Explainer(autoencoder.predict, X_anom)\n",
    "\n",
    "# # Calcula valores SHAP\n",
    "# shap_values = explainer(X_anom)\n",
    "\n",
    "# # Visualização\n",
    "# shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0668b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
